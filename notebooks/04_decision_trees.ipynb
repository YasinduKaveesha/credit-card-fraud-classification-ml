{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fe36183-aad8-4459-8ed5-1e63deca03e0",
   "metadata": {},
   "source": [
    "# Baseline Models: Decision Trees\n",
    "\n",
    "## Objective\n",
    "Evaluate non-linear tree-based models for credit card fraud detection and\n",
    "understand the trade-off between model complexity, precision, and recall\n",
    "under extreme class imbalance.\n",
    "\n",
    "This notebook compares:\n",
    "- A **shallow Decision Tree** (controlled complexity)\n",
    "- A **fully grown Decision Tree** (high variance model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e7a376d-05ed-4ec4-808e-deb4dc0cb6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9047ff73-f0e4-4e76-b826-85e1a0eb0b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data\n",
    "X_train = np.load(\"../data/processed/v1_train_test/X_train_scaled.npy\")\n",
    "X_test  = np.load(\"../data/processed/v1_train_test/X_test_scaled.npy\")\n",
    "y_train = np.load(\"../data/processed/v1_train_test/y_train.npy\")\n",
    "y_test  = np.load(\"../data/processed/v1_train_test/y_test.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "632c5fb0-5dd3-4fd6-99a7-099a74a151ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classifier(name, y_true, y_pred, y_proba):\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(\"ROC-AUC:\", roc_auc_score(y_true, y_proba))\n",
    "    print(\"PR-AUC :\", average_precision_score(y_true, y_proba))\n",
    "    print(\"Confusion Matrix [ [TN FP], [FN TP] ]:\")\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    print(classification_report(y_true, y_pred, digits=4, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8acc458-da33-4022-bc69-0cfdf5198e21",
   "metadata": {},
   "source": [
    "## Shallow Decision Tree (max_depth = 5)\n",
    "\n",
    "A constrained tree is used to:\n",
    "- limit overfitting\n",
    "- capture basic non-linear interactions\n",
    "- serve as a controlled non-linear baseline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e416b6b-4e17-4a3a-9e50-cb6ee7ffe3f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Decision Tree (max_depth=5) ===\n",
      "ROC-AUC: 0.9165621985288207\n",
      "PR-AUC : 0.44978383369250374\n",
      "Confusion Matrix [ [TN FP], [FN TP] ]:\n",
      "[[55135  1729]\n",
      " [   12    86]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9998    0.9696    0.9845     56864\n",
      "           1     0.0474    0.8776    0.0899        98\n",
      "\n",
      "    accuracy                         0.9694     56962\n",
      "   macro avg     0.5236    0.9236    0.5372     56962\n",
      "weighted avg     0.9981    0.9694    0.9829     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tree = DecisionTreeClassifier(\n",
    "    max_depth=5,\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "tree_pred  = tree.predict(X_test)\n",
    "tree_proba = tree.predict_proba(X_test)[:, 1]\n",
    "\n",
    "evaluate_classifier(\"Decision Tree (max_depth=5)\", y_test, tree_pred, tree_proba)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfe8039-722f-4921-9fba-c5d37bbd2bd9",
   "metadata": {},
   "source": [
    "## Fully Grown Decision Tree (No max_depth)\n",
    "\n",
    "This model allows unrestricted growth and is used to demonstrate:\n",
    "- high variance behavior\n",
    "- overfitting risk in imbalanced datasets\n",
    "- precisionâ€“recall trade-offs at extreme complexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "851f73ad-42b8-462d-8911-1ad4143372db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Decision Tree (no max_depth) ===\n",
      "ROC-AUC: 0.8619459390396564\n",
      "PR-AUC : 0.4903671003078485\n",
      "Confusion Matrix [ [TN FP], [FN TP] ]:\n",
      "[[56830    34]\n",
      " [   27    71]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9995    0.9994    0.9995     56864\n",
      "           1     0.6762    0.7245    0.6995        98\n",
      "\n",
      "    accuracy                         0.9989     56962\n",
      "   macro avg     0.8379    0.8619    0.8495     56962\n",
      "weighted avg     0.9990    0.9989    0.9989     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deep_tree = DecisionTreeClassifier(\n",
    "    max_depth=None,\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "deep_tree.fit(X_train, y_train)\n",
    "\n",
    "dt_pred  = deep_tree.predict(X_test)\n",
    "dt_proba = deep_tree.predict_proba(X_test)[:, 1]\n",
    "\n",
    "evaluate_classifier(\"Decision Tree (no max_depth)\", y_test, dt_pred, dt_proba)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94de8ffb-128d-4758-bd85-f55a6be6778c",
   "metadata": {},
   "source": [
    "## Model Comparison & Interpretation\n",
    "\n",
    "- The shallow tree achieves high recall but suffers from very low precision,\n",
    "  resulting in many false positives.\n",
    "- The deep tree significantly improves precision but loses recall,\n",
    "  indicating overfitting to specific fraud patterns.\n",
    "- This highlights the instability of single-tree models under severe\n",
    "  class imbalance.\n",
    "\n",
    "These results motivate the use of **ensemble methods** such as\n",
    "Random Forests and Gradient Boosting, which can balance bias and variance\n",
    "more effectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99e57d9-d8dd-4951-b4dd-a90d5aa07758",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
